<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Super Fast String Matching in Python</title>
  <meta name="description" content="Traditional approaches to string matching such as the Jaro-Winkler or Levenshtein distance measure are too slow for large datasets. Using TF-IDF with N-Grams...">

  
  
  <link rel="stylesheet" href="http://localhost:4000/assets/style.css">

  <link rel="canonical" href="http://localhost:4000/2017/10/14/super-fast-string-matching.html">
  <link rel="alternate" type="application/rss+xml" title="van den Blog" href="http://localhost:4000/feed.xml">

  <script async defer src="https://buttons.github.io/buttons.js"></script>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-105892364-1', 'auto');
  ga('send', 'pageview');

</script>
</head>


  <body>

    <header class="border-bottom-thick px-2 clearfix">
  <div class="left sm-width-full py-1 mt-1 mt-lg-0">
    <a class="align-middle link-primary text-accent" href="/">
      van den Blog
    </a>
  </div>
  <div class="right sm-width-full">
    <ul class="list-reset mt-lg-1 mb-2 mb-lg-1">
      
        
        <li class="inline-block">
          <a class="align-middle link-primary mr-2 mr-lg-0 ml-lg-2" href="/about/">
            About
          </a>
        </li>
        
      
        
      
        
      
        
        <li class="inline-block">
          <a class="align-middle link-primary mr-2 mr-lg-0 ml-lg-2" href="/resume.html">
            Resume
          </a>
        </li>
        
      
        
      
    </ul>
  </div>
</header>


    <div>
      <article class="container px-2 mx-auto mb4" itemscope itemtype="http://schema.org/BlogPosting">
  <h1 class="h0 col-9 sm-width-full py-4 mt-3 inline-block" itemprop="name headline">Super Fast String Matching in Python</h1>
  <div class="col-4 sm-width-full mt-1 border-top-thin ">
    <p class="mb-3 py-2 bold h4"><time datetime="2017-10-14T00:00:00-04:00" itemprop="datePublished">Oct 14, 2017</time></p>
  </div>

  <div class="prose" itemprop="articleBody">
      <p>Traditional approaches to string matching such as the <a href="https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance">Jaro-Winkler</a> or <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein</a> distance measure are too slow for large datasets. Using TF-IDF with N-Grams as terms
 to find similar strings transforms the problem into a matrix multiplication problem, which is computationally much cheaper. Using this approach made it possible to search for near duplicates in a set of 663,000 company names in 42 minutes using only a dual-core laptop.</p>

<h2 id="name-matching">Name Matching</h2>

<p>A problem that I have witnessed many times, and I think many other people working with databases with me,
 is that of name matching. Databases often have multiple entries that relate to the same entity, for example a person 
 or company, where one entry has a slightly different spelling then the other. This becomes a problem, and 
 you want to de-duplicate these. A similar problem occurs when you want to merge or join databases 
 using the names as identifier.</p>

<p>The following table gives an example:</p>

<table>
  <thead>
    <tr>
      <th>Company Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Burger King</td>
    </tr>
    <tr>
      <td>Mc Donalds</td>
    </tr>
    <tr>
      <td>KFC</td>
    </tr>
    <tr>
      <td>Mac Donald’s</td>
    </tr>
  </tbody>
</table>

<p>For the human reader it is obvious that both <em>Mc Donalds</em> and <em>Mac Donald’s</em> are the same company, however for a computer these are completely different making spotting these nearly identical strings difficult.</p>

<p>One way to solve this would be by using a string similarity measures like <a href="https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance">Jaro-Winkler</a> or the <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein</a> distance measure. The obvious problem here is that the amount of calculations necessary grow quadratic. Every entry has to be compared with every other entry in the dataset, in our case this means calculating one of these measures 663.000^2 times. In this post I will explain how to do this can be done faster using TF-IDF, N-Grams, and sparse matrix multiplication.</p>

<h2 id="the-dataset">The Dataset</h2>

<p>I just grabbed a random dataset with lots of company names from <a href="https://www.kaggle.com/dattapiy/sec-edgar-companies-list">Kaggle</a>. It contains all company names in the SEC EDGAR database. I don’t know anything about the data or the amount of duplicates in this dataset (there should be 0), but most likely there will be some very similar names.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_colwidth'</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">names</span> <span class="o">=</span>  <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/sec_edgar_company_info.csv'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'The shape: </span><span class="si">%</span><span class="s">d x </span><span class="si">%</span><span class="s">d'</span> <span class="o">%</span> <span class="n">names</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">names</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>The shape: 663000 x 3
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: center;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th></th>
      <th>Line Number</th>
      <th>Company Name</th>
      <th>Company CIK Key</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>!J INC</td>
      <td>1438823</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>#1 A LIFESAFER HOLDINGS, INC.</td>
      <td>1509607</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>#1 ARIZONA DISCOUNT PROPERTIES LLC</td>
      <td>1457512</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>#1 PAINTBALL CORP</td>
      <td>1433777</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>$ LLC</td>
      <td>1427189</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="tf-idf">TF-IDF</h2>

<p>TF-IDF is a method to generate features from text by multiplying the frequency of a term (usually a word) in a document (the <em>Term Frequency</em>, or <em>TF</em>) by the importance (the <em>Inverse Document Frequency</em> or <em>IDF</em>) of the same term in an entire corpus. This last term weights less important words (e.g. the, it, and etc) down, and words that don’t occur frequently up. IDF is calculated as:</p>

<p><code class="highlighter-rouge">IDF(t) = log_e(Total number of documents / Number of documents with term t in it).</code></p>

<p>An example (from <a href="http://www.tfidf.com/">www.tfidf.com/</a>):</p>

<p>Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.</p>

<p>TF-IDF is very useful in text classification and text clustering. It is used to transform documents into numeric 
vectors, that can easily be compared.</p>

<h2 id="n-grams">N-Grams</h2>

<p>While the terms in TF-IDF are usually words, this is not a necessity. In our case using words as terms wouldn’t help us much, as most company names only contain one or two words. This is why we will use <em>n-grams</em>: sequences of <em>N</em>  contiguous items, in this case characters. The following function cleans a string and generates all n-grams in this string:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">ngrams</span><span class="p">(</span><span class="n">string</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">string</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r'[,|-|.|/|  BD]'</span><span class="p">,</span><span class="s">r''</span><span class="p">,</span> <span class="n">string</span><span class="p">)</span>
    <span class="n">ngrams</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">string</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
    <span class="k">return</span> <span class="p">[</span><span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ngram</span><span class="p">)</span> <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">'All 3-grams in "McDonalds":'</span><span class="p">)</span>
<span class="n">ngrams</span><span class="p">(</span><span class="s">'McDonalds'</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>All 3-grams in "McDonalds":

['Mco', 'con', 'ona', 'nal', 'ald', 'lds']
</code></pre>
</div>

<p>As you can see, the code above does some cleaning as well. Next to removing some punctuation (dots, comma’s etc) it removes the string “ BD”. This is a nice example of one of the pitfalls of this approach: some terms that appear very infrequent will result in a high bias towards this term. In this case there where some company names ending with “ BD” that where being identified as similar, even though the rest of the string was not similar.</p>

<p>The code to generate the matrix of TF-IDF values for each is shown below.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">company_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">[</span><span class="s">'Company Name'</span><span class="p">]</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">analyzer</span><span class="o">=</span><span class="n">ngrams</span><span class="p">)</span>
<span class="n">tf_idf_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">company_names</span><span class="p">)</span>
</code></pre>
</div>

<p>The resulting matrix is very sparse as most terms in the corpus will not appear in most company names. Scikit-learn deals with this nicely by returning a sparse <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html">CSR  matrix</a>.</p>

<p>You can see the first row (“!J INC”) contains three terms for the columns 11, 16196, and 15541.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">tf_idf_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c"># Check if this makes sense:</span>

<span class="n">ngrams</span><span class="p">(</span><span class="s">'!J INC'</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>  (0, 11)	0.844099068282
  (0, 16196)	0.51177784466
  (0, 15541)	0.159938115034

['!JI', 'JIN', 'INC']
</code></pre>
</div>

<p>The last term (‘INC’) has a reletivly low value, which makes sense as this term will appear often in the corpus, thus recieving a lower IDF weight.</p>

<h2 id="cosine-similarity">Cosine Similarity</h2>

<p>To calculate the similarity between two vectors of TF-IDF values the <em>Cosine Similarity</em> is usually used. 
The cosine similarity can be seen as a normalized dot product. 
For a good explanation see: <a href="http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/">this site</a>. 
We can theoretically calculate the cosine similarity of all items in our dataset with all other items 
in scikit-learn by using the cosine_similarity function, however the Data Scientists 
at ING found out this has <a href="https://medium.com/@ingwbaa/https-medium-com-ingwbaa-boosting-selection-of-the-most-similar-entities-in-large-scale-datasets-450b3242e618">some disadvantages</a>:</p>

<ul>
  <li>The sklearn version does a lot of type checking and error handling.</li>
  <li>The sklearn version calculates and stores all similarities in one go, while we are only interested in the most similar ones. Therefore it uses a lot more memory than necessary.</li>
</ul>

<p>To optimize for these disadvantages they created their <a href="https://github.com/ing-bank/sparse_dot_topn">own library</a> which stores only the top N highest matches in each row, and only the similarities above an (optional) threshold.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">csr_matrix</span>
<span class="kn">import</span> <span class="nn">sparse_dot_topn.sparse_dot_topn</span> <span class="kn">as</span> <span class="nn">ct</span>

<span class="k">def</span> <span class="nf">awesome_cossim_top</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">ntop</span><span class="p">,</span> <span class="n">lower_bound</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c"># force A and B as a CSR matrix.</span>
    <span class="c"># If they have already been CSR, there is no overhead</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span>
 
    <span class="n">idx_dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span>
 
    <span class="n">nnz_max</span> <span class="o">=</span> <span class="n">M</span><span class="o">*</span><span class="n">ntop</span>
 
    <span class="n">indptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">idx_dtype</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nnz_max</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">idx_dtype</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nnz_max</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">A</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">ct</span><span class="o">.</span><span class="n">sparse_dot_topn</span><span class="p">(</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">idx_dtype</span><span class="p">),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">idx_dtype</span><span class="p">),</span>
        <span class="n">A</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">idx_dtype</span><span class="p">),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">idx_dtype</span><span class="p">),</span>
        <span class="n">B</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
        <span class="n">ntop</span><span class="p">,</span>
        <span class="n">lower_bound</span><span class="p">,</span>
        <span class="n">indptr</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">csr_matrix</span><span class="p">((</span><span class="n">data</span><span class="p">,</span><span class="n">indices</span><span class="p">,</span><span class="n">indptr</span><span class="p">),</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="n">N</span><span class="p">))</span>
</code></pre>
</div>

<p>The following code runs the optimized cosine similarity function. It only stores the top 10 most similar items, and only items with a similarity above 0.8:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">time</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">matches</span> <span class="o">=</span> <span class="n">awesome_cossim_top</span><span class="p">(</span><span class="n">tf_idf_matrix</span><span class="p">,</span> <span class="n">tf_idf_matrix</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t1</span>
<span class="k">print</span><span class="p">(</span><span class="s">"SELFTIMED:"</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>SELFTIMED: 2718.7523670196533
</code></pre>
</div>

<p>The following code unpacks the resulting sparse matrix. As it is a bit slow, an option to look at only the first <em>n</em> values is added.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_matches_df</span><span class="p">(</span><span class="n">sparse_matrix</span><span class="p">,</span> <span class="n">name_vector</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">non_zeros</span> <span class="o">=</span> <span class="n">sparse_matrix</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
    
    <span class="n">sparserows</span> <span class="o">=</span> <span class="n">non_zeros</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">sparsecols</span> <span class="o">=</span> <span class="n">non_zeros</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">if</span> <span class="n">top</span><span class="p">:</span>
        <span class="n">nr_matches</span> <span class="o">=</span> <span class="n">top</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">nr_matches</span> <span class="o">=</span> <span class="n">sparsecols</span><span class="o">.</span><span class="n">size</span>
    
    <span class="n">left_side</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">nr_matches</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="n">right_side</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">nr_matches</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="n">similairity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nr_matches</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nr_matches</span><span class="p">):</span>
        <span class="n">left_side</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">name_vector</span><span class="p">[</span><span class="n">sparserows</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span>
        <span class="n">right_side</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">name_vector</span><span class="p">[</span><span class="n">sparsecols</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span>
        <span class="n">similairity</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sparse_matrix</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'left_side'</span><span class="p">:</span> <span class="n">left_side</span><span class="p">,</span>
                          <span class="s">'right_side'</span><span class="p">:</span> <span class="n">right_side</span><span class="p">,</span>
                           <span class="s">'similairity'</span><span class="p">:</span> <span class="n">similairity</span><span class="p">})</span>
        
</code></pre>
</div>

<p>Lets look at our matches:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">matches_df</span> <span class="o">=</span> <span class="n">get_matches_df</span><span class="p">(</span><span class="n">matches</span><span class="p">,</span> <span class="n">company_names</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
<span class="n">matches_df</span> <span class="o">=</span> <span class="n">matches_df</span><span class="p">[</span><span class="n">matches_df</span><span class="p">[</span><span class="s">'similairity'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">0.99999</span><span class="p">]</span> <span class="c"># Remove all exact matches</span>
<span class="n">matches_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</code></pre>
</div>

<div>
<table class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th></th>
      <th>left_side</th>
      <th>right_side</th>
      <th>similairity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>41024</th>
      <td>ADVISORY U S EQUITY MARKET NEUTRAL OVERSEAS FUND LTD</td>
      <td>ADVISORY US EQUITY MARKET NEUTRAL FUND LP</td>
      <td>0.818439</td>
    </tr>
    <tr>
      <th>48061</th>
      <td>AIM VARIABLE INSURANCE FUNDS</td>
      <td>AIM VARIABLE INSURANCE FUNDS (INVESCO VARIABLE INSURANCE FUNDS)</td>
      <td>0.856922</td>
    </tr>
    <tr>
      <th>14978</th>
      <td>ACP ACQUISITION CORP</td>
      <td>CP ACQUISITION CORP</td>
      <td>0.913479</td>
    </tr>
    <tr>
      <th>54837</th>
      <td>ALLFIRST TRUST CO NA</td>
      <td>ALLFIRST TRUST CO NA                         /TA/</td>
      <td>0.938206</td>
    </tr>
    <tr>
      <th>89788</th>
      <td>ARMSTRONG MICHAEL L</td>
      <td>ARMSTRONG MICHAEL</td>
      <td>0.981860</td>
    </tr>
    <tr>
      <th>54124</th>
      <td>ALLEN MICHAEL D</td>
      <td>ALLEN MICHAEL J</td>
      <td>0.928606</td>
    </tr>
    <tr>
      <th>66765</th>
      <td>AMERICAN SCRAP PROCESSING INC</td>
      <td>SCRAP PROCESSING INC</td>
      <td>0.858714</td>
    </tr>
    <tr>
      <th>44886</th>
      <td>AGL LIFE ASSURANCE CO SEPARATE ACCOUNT VA 27</td>
      <td>AGL LIFE ASSURANCE CO SEPARATE ACCOUNT VA 24</td>
      <td>0.880202</td>
    </tr>
    <tr>
      <th>49119</th>
      <td>AJW PARTNERS II LLC</td>
      <td>AJW PARTNERS LLC</td>
      <td>0.876761</td>
    </tr>
    <tr>
      <th>16712</th>
      <td>ADAMS MICHAEL C.</td>
      <td>ADAMS MICHAEL A</td>
      <td>0.891636</td>
    </tr>
    <tr>
      <th>96207</th>
      <td>ASTRONOVA, INC.</td>
      <td>PETRONOVA, INC.</td>
      <td>0.841667</td>
    </tr>
    <tr>
      <th>26079</th>
      <td>ADVISORS DISCIPLINED TRUST 1329</td>
      <td>ADVISORS DISCIPLINED TRUST 1327</td>
      <td>0.862806</td>
    </tr>
    <tr>
      <th>16200</th>
      <td>ADAMANT TECHNOLOGIES</td>
      <td>NT TECHNOLOGIES, INC.</td>
      <td>0.814618</td>
    </tr>
    <tr>
      <th>77473</th>
      <td>ANGELLIST-SORY-FUND, A SERIES OF ANGELLIST-SDA-FUNDS, LLC</td>
      <td>ANGELLIST-NABS-FUND, A SERIES OF ANGELLIST-SDA-FUNDS, LLC</td>
      <td>0.828394</td>
    </tr>
    <tr>
      <th>70624</th>
      <td>AN STD ACQUISITION CORP</td>
      <td>OT ACQUISITION CORP</td>
      <td>0.855598</td>
    </tr>
    <tr>
      <th>16669</th>
      <td>ADAMS MARK B</td>
      <td>ADAMS MARY C</td>
      <td>0.812897</td>
    </tr>
    <tr>
      <th>48371</th>
      <td>AIR SEMICONDUCTOR INC</td>
      <td>LION SEMICONDUCTOR INC.</td>
      <td>0.814091</td>
    </tr>
    <tr>
      <th>53755</th>
      <td>ALLEN DANIEL M.</td>
      <td>ALLEN DANIEL J</td>
      <td>0.829631</td>
    </tr>
    <tr>
      <th>16005</th>
      <td>ADA EMERGING MARKETS FUND, LP</td>
      <td>ORANDA EMERGING MARKETS FUND LP</td>
      <td>0.839016</td>
    </tr>
    <tr>
      <th>97135</th>
      <td>ATHENE ASSET MANAGEMENT LLC</td>
      <td>CRANE ASSET MANAGEMENT LLC</td>
      <td>0.807580</td>
    </tr>
  </tbody>
</table>
</div>

<p>The matches look pretty similar! The cossine similarity gives a good indication of the similarity between the two company names. <em>ATHENE ASSET MANAGEMENT LLC</em> and <em>CRANE ASSET MANAGEMENT LLC</em> are probably not the same company, and the similarity measure of 0.81 reflects this. When we look at the company names with the highest similarity, we see that these are pretty long strings that differ by only 1 character:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">matches_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">([</span><span class="s">'similairity'</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: center;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>left_side</th>
      <th>right_side</th>
      <th>similairity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>77993</th>
      <td>ANGLE LIGHT CAPITAL, LP - ANGLE LIGHT CAPITAL - QUASAR SERIES I</td>
      <td>ANGLE LIGHT CAPITAL, LP - ANGLE LIGHT CAPITAL - QUASAR SERIES II</td>
      <td>0.994860</td>
    </tr>
    <tr>
      <th>77996</th>
      <td>ANGLE LIGHT CAPITAL, LP - ANGLE LIGHT CAPITAL - QUASAR SERIES II</td>
      <td>ANGLE LIGHT CAPITAL, LP - ANGLE LIGHT CAPITAL - QUASAR SERIES I</td>
      <td>0.994860</td>
    </tr>
    <tr>
      <th>81120</th>
      <td>APOLLO OVERSEAS PARTNERS (DELAWARE 892) VIII, L.P.</td>
      <td>APOLLO OVERSEAS PARTNERS (DELAWARE 892) VII LP</td>
      <td>0.993736</td>
    </tr>
    <tr>
      <th>81116</th>
      <td>APOLLO OVERSEAS PARTNERS (DELAWARE 892) VII LP</td>
      <td>APOLLO OVERSEAS PARTNERS (DELAWARE 892) VIII, L.P.</td>
      <td>0.993736</td>
    </tr>
    <tr>
      <th>66974</th>
      <td>AMERICAN SKANDIA LIFE ASSURANCE CORP VARIABLE ACCOUNT E</td>
      <td>AMERICAN SKANDIA LIFE ASSURANCE CORP VARIABLE ACCOUNT B</td>
      <td>0.993527</td>
    </tr>
    <tr>
      <th>66968</th>
      <td>AMERICAN SKANDIA LIFE ASSURANCE CORP VARIABLE ACCOUNT B</td>
      <td>AMERICAN SKANDIA LIFE ASSURANCE CORP VARIABLE ACCOUNT E</td>
      <td>0.993527</td>
    </tr>
    <tr>
      <th>80929</th>
      <td>APOLLO EUROPEAN PRINCIPAL FINANCE FUND III (EURO B), L.P.</td>
      <td>APOLLO EUROPEAN PRINCIPAL FINANCE FUND II (EURO B), L.P.</td>
      <td>0.993375</td>
    </tr>
    <tr>
      <th>80918</th>
      <td>APOLLO EUROPEAN PRINCIPAL FINANCE FUND II (EURO B), L.P.</td>
      <td>APOLLO EUROPEAN PRINCIPAL FINANCE FUND III (EURO B), L.P.</td>
      <td>0.993375</td>
    </tr>
    <tr>
      <th>80921</th>
      <td>APOLLO EUROPEAN PRINCIPAL FINANCE FUND III (DOLLAR A), L.P.</td>
      <td>APOLLO EUROPEAN PRINCIPAL FINANCE FUND II (DOLLAR A), L.P.</td>
      <td>0.993116</td>
    </tr>
    <tr>
      <th>80907</th>
      <td>APOLLO EUROPEAN PRINCIPAL FINANCE FUND II (DOLLAR A), L.P.</td>
      <td>APOLLO EUROPEAN PRINCIPAL FINANCE FUND III (DOLLAR A), L.P.</td>
      <td>0.993116</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="conclusion">Conclusion</h2>

<p>As we saw by visual inspection the matches created with this method are quite good, as the strings are very similar. 
The biggest advantage is the speed however. The method described above can be scaled to much larger
 datasets by using a distributed computing environment such as Apache Spark. This could be done by broadcasting 
 one of the TF-IDF matrices to all workers, and parallelizing the second (in our case a copy of the TF-IDF matrix) 
 into multiple sub-matrices. Multiplication can then be done (using Numpy or the sparse_dot_topn library) 
 by each worker on part of the second matrix and the entire first matrix. 
 An example of this is described <a href="https://labs.yodas.com/large-scale-matrix-multiplication-with-pyspark-or-how-to-match-two-large-datasets-of-company-1be4b1b2871e">here</a>.</p>

  </div>

</article>

<div class="container mx-auto px-2 py-2 clearfix">
  <!-- Use if you want to show previous and next for all posts. -->



  <div class="col-4 sm-width-full left mr-lg-4 mt-3">
    <a class="no-underline border-top-thin py-1 block" href="http://localhost:4000/2017/08/29/citi-bike-kde.html">
      <span class="h5 bold text-accent">Previous</span>
      <p class="bold h3 link-primary mb-1">1 Day of Citi Bike availability </p>
      <p>After moving to New York from the Netherlands I was relieved to find out that biking in Manhattan is actually...</p>
    </a>
  </div>
  
  

</div>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-105892364-1', 'auto');
  ga('send', 'pageview');

</script>
    </div>

    <div class="border-top-thin clearfix mt-2 mt-lg-4">
  <div class="container mx-auto px-2">
    <p class="col-8 sm-width-full left py-2 mb-0">This project is maintained by <a class="text-accent" href="https://github.com/bergvca">bergvca</a></p>
    <ul class="list-reset right clearfix sm-width-full py-2 mb-2 mb-lg-0">
      <li class="inline-block mr-1">
        <a href="https://twitter.com/share" class="twitter-share-button" data-hashtags="van den Blog">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
      </li>
      <li class="inline-block">
        <a class="github-button" href="https://github.com/bergvca/" data-icon="octicon-star" data-count-href="bergvca//stargazers" data-count-api="/repos/bergvca/#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star bergvca/ on GitHub">Star</a>
      </li>
    </ul>
  </div>
</div>


  </body>

</html>
